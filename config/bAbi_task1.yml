data:
  base_path: 'data/'
  task_path: 'en-10k/'
  task_id: 1
  PAD_ID: 0

model:
  batch_size: 16
  use_pretrained: true             # (true or false)
  embed_dim: 50                    # if use_pretrained: only available 50, 100, 200, 300
  encoder_type: UNI                # UNI, BI
  cell_type: GRU                   # LSTM, GRU, layer_norm_lstm, nas
  num_layers: 1
  num_units: 32
  dropout: 0.0
  reg_scale: 0.001
  cap_grads: true
  max_grad_val: 5
  noisy_grads: true
  num_hops: 3

train:
  learning_rate: 0.0001
  optimizer: 'Adam'                # Adagrad, Adam, Ftrl, Momentum, RMSProp, SGD

  train_steps: 100000
  model_dir: 'logs/bAbi_task1'

  save_checkpoints_steps: 1000
  check_hook_n_iter: 1000
  min_eval_frequency: 1000

  print_verbose: False
  debug: False
  log_step_count: 500
  tf_random_seed: 123456
  log_step_count_steps: 500
  n_epochs: 1
  CUDA_VISIBLE_DEVICES: "0,1,2"

